{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh6QOr-qO4Ym"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/education-toolkit/blob/main/02_ml-demos-with-gradio.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "üí° **Welcome!**\n",
    "\n",
    "We‚Äôve assembled a toolkit that university instructors and organizers can use to easily prepare labs, homework, or classes. The content is designed in a self-contained way such that it can easily be incorporated into the existing curriculum. This content is free and uses widely known Open Source technologies (`transformers`, `gradio`, etc).\n",
    "\n",
    "Alternatively, you can request for someone on the Hugging Face team to run the tutorials for your class via the [ML demo.cratization tour](https://huggingface2.notion.site/ML-Demo-cratization-tour-with-66847a294abd4e9785e85663f5239652) initiative!\n",
    "\n",
    "You can find all the tutorials and resources we‚Äôve assembled [here](https://huggingface2.notion.site/Education-Toolkit-7b4a9a9d65ee4a6eb16178ec2a4f3599). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkJmA-r5L0EB"
   },
   "source": [
    "# Tutorial: Build and Host Machine Learning Demos with Gradio ‚ö° & Hugging Face ü§ó "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_Iv1CJZPekG"
   },
   "source": [
    "**Learning goals:** The goal of this tutorial is to learn How To\n",
    "\n",
    "1. Build a quick demo for your machine learning model in Python using the `gradio` library\n",
    "2. Host the demos for free with Hugging Face Spaces\n",
    "3. Add your demo to the Hugging Face org for your class or conference. This includes:\n",
    "  *   A setup step for instructors (or conference organizers)\n",
    "  *   Upload instructions for students (or conference participants)\n",
    "\n",
    "**Duration**: 20-40\n",
    " minutes\n",
    "\n",
    "**Prerequisites:** Knowledge of Python and basic familiarity with machine learning \n",
    "\n",
    "\n",
    "**Author**: [Abubakar Abid](https://twitter.com/abidlabs) (feel free to ping me with any questions about this tutorial) \n",
    "\n",
    "All of these steps can be done for free! All you need is an Internet browser and a place where you can write Python üë©‚Äçüíª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PR9faV2NWTrG"
   },
   "source": [
    "## Why Demos?\n",
    "\n",
    "**Demos** of machine learning models are an increasingly important part of machine learning _courses_ and _conferences_. Demos allow:\n",
    "\n",
    "* model developers to easily **present** their work to a wide audience\n",
    "* increase **reproducibility** of machine learning research\n",
    "* diverse users to more easily **identify and debug** failure points of models\n",
    "\n",
    "\n",
    "As a quick example of what we would like to build, check out the [Keras Org on Hugging Face](https://huggingface.co/keras-io), which includes a description card and a collection of Models and Spaces built by Keras community. Any Space can be opened in your browser and you can use the model immediately, as shown here: \n",
    "\n",
    "![](https://i.ibb.co/7y6DGjB/ezgif-5-cc52b7e590.gif)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0KzbU4lQtv3"
   },
   "source": [
    "## 1. Build Quick ML Demos in Python Using the Gradio Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlSs72oUQ1VW"
   },
   "source": [
    "`gradio` is a handy Python library that lets you build web demos simply by specifying the list of input and output **components** expected by your machine learning model. \n",
    "\n",
    "What do I mean by input and output components? Gradio comes with a bunch of predefined components for different kinds of machine learning models. Here are some examples:\n",
    "\n",
    "* For an **image classifier**, the expected input type is an `Image` and the output type is a `Label`. \n",
    "* For a **speech recognition model**, the expected input component is an `Microphone` (which lets users record from the browser) or `Audio` (which lets users drag-and-drop audio files), while the output type is `Text`. \n",
    "* For a **question answering model**, we expect **2 inputs**: [`Text`, `Text`], one textbox for the paragraph and one for the question, and the output type is a single `Text` corresponding to the answer. \n",
    "\n",
    "You get the idea... (for all of the supported components, [see the docs](https://gradio.app/docs/))\n",
    "\n",
    "In addition to the input and output types, Gradio expects a third parameter, which is the prediction function itself. This parameter can be ***any* regular Python function** that takes in parameter(s) corresponding to the input component(s) and returns value(s) corresponding to the output component(s)\n",
    "\n",
    "Enough words. Let's see some code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0MkPbbZbSiP",
    "outputId": "e143c5df-5b98-46c6-f2f7-7fc7abebd3d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 871 kB 5.1 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.0 MB 41.5 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52 kB 787 kB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 25.8 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52 kB 1.1 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210 kB 56.5 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94 kB 2.8 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271 kB 58.7 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144 kB 58.8 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.9 MB 44.8 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58 kB 5.3 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79 kB 6.6 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 856 kB 60.6 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61 kB 374 kB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.6 MB 50.0 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58 kB 4.5 MB/s \n",
      "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# First, install Gradio\n",
    "!pip install --quiet gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SjTxhry8bWS7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sepia(image):\n",
    "    sepia_filter = np.array(\n",
    "        [[0.393, 0.769, 0.189], \n",
    "         [0.349, 0.686, 0.168], \n",
    "         [0.272, 0.534, 0.131]]\n",
    "    )\n",
    "    sepia_img = image.dot(sepia_filter.T)\n",
    "    sepia_img /= sepia_img.max()\n",
    "    return sepia_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgqlIG2DbrJq"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Write 1 line of Python to create a simple GUI\n",
    "gr.Interface(fn=sepia, inputs=\"image\", outputs=\"image\").launch(share=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TyTGpSsb7bs"
   },
   "source": [
    "Running the code above should produce a simple GUI inside this notebook allowing you to type example inputs and see the output returned by your function. \n",
    "\n",
    "Notice that we define an `Interface` using the 3 ingredients mentioned earlier:\n",
    "* A function\n",
    "* Input component(s)\n",
    "* Output component(s)\n",
    "\n",
    "This is a simple example for images, but the same principle holds true for any other kind of data type. For example, here is an interface that generates a musical tone when provided a few different parameters (the specific code inside `generate_tone()` is not important for the purpose of this tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "id": "cHiZAO6ub6kA",
    "outputId": "ee9e8bfd-4b86-4ddf-c96d-d389cdc0730e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
      "Running on public URL: https://20619.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://20619.gradio.app\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f84a6eaeb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<fastapi.applications.FastAPI at 0x7f84a8c6f850>,\n",
       " 'http://127.0.0.1:7860/',\n",
       " 'https://20619.gradio.app')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def generate_tone(note, octave, duration):\n",
    "    sampling_rate = 48000\n",
    "    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)\n",
    "    frequency = a4_freq * 2 ** (tones_from_a4 / 12)\n",
    "    audio = np.linspace(0, int(duration), int(duration) * sampling_rate)\n",
    "    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)\n",
    "    return sampling_rate, audio\n",
    "\n",
    "gr.Interface(\n",
    "    generate_tone,\n",
    "    [\n",
    "        gr.inputs.Dropdown([\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"], type=\"index\"),\n",
    "        gr.inputs.Slider(4, 6, step=1),\n",
    "        gr.inputs.Textbox(type=\"number\", default=1, label=\"Duration in seconds\"),\n",
    "    ],\n",
    "    \"audio\",\n",
    "    title=\"Generate a Musical Tone!\"\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23gD280-w-kT"
   },
   "source": [
    "**Challenge #1**: build a Gradio demo that takes in an image and returns the same image *flipped upside down* in less than 10 lines of Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSE6TZF5e9Oz"
   },
   "source": [
    "There are a lot more examples you can try in Gradio's [getting started page](https://gradio.app/getting_started/), which cover additional features such as:\n",
    "* Adding example inputs\n",
    "* Adding _state_ (e.g. for chatbots)\n",
    "* Sharing demos easily using one parameter called `share` (<-- this is pretty cool üòé)\n",
    "\n",
    "It is especially easy to demo a `transformers` model from Hugging Face's Model Hub, using the special `gr.Interface.load` method. \n",
    "\n",
    "Let's try a text-to-speech model built by Facebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Interface.load(\"huggingface/facebook/fastspeech2-en-ljspeech\").launch(share=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to build a demo for [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B), a large language model & add a couple of examples inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "N_Cobhx8e8v9",
    "outputId": "2bac3837-feff-42ea-a577-60343f19535b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching model from: https://huggingface.co/EleutherAI/gpt-j-6B\n",
      "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
      "Running on public URL: https://30262.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://30262.gradio.app\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1a12d753d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "examples = [[\"The Moon's orbit around Earth has\"], [\"There once was a pineapple\"]]\n",
    "\n",
    "gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\", examples=examples).launch(share=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoUYf0rYksA9"
   },
   "source": [
    "**Challenge #2**: Go to the [Hugging Face Model Hub](https://huggingface.co/models), and pick a model that performs one of the other tasks supported in the `transformers` library (other than the two you just saw: text generation or text-to-speech). Create a Gradio demo for that model using `gr.Interface.load`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6Ek7cORgDkQ"
   },
   "source": [
    "## 2. Host the Demo (for free) on Hugging Face Spaces\n",
    "\n",
    "Once you made a Gradio demo, you can host it permanently on Hugging Spaces very easily:\n",
    "\n",
    "Here are the steps to that (shown in the GIF below):\n",
    "\n",
    "A. First, create a Hugging Face account if you do not already have one, by visiting https://huggingface.co/ and clicking \"Sign Up\"\n",
    "\n",
    "B. Once you are logged in, click on your profile picture and then click on \"New Space\" underneath it to get to this page: https://huggingface.co/new-space\n",
    "\n",
    "C. Give your Space a name and a license. Select \"Gradio\" as the Space SDK, and then choose \"Public\" if you are fine with everyone accessing your Space and the underlying code\n",
    "\n",
    "D. Then you will find a page that provides you instructions on how to upload your files into the Git repository for that Space. You may also need to add a `requirements.txt` file to specify any Python package dependencies.\n",
    "\n",
    "E. Once you have pushed your files, that's it! Spaces will automatically build your Gradio demo allowing you to share it with anyone, anywhere!\n",
    "\n",
    "![GIF](https://huggingface.co/blog/assets/28_gradio-spaces/spaces-demo-finalized.gif)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4XCmQ_RILoq"
   },
   "source": [
    "You can even embed your Gradio demo on any website -- in a blog, a portfolio page, or even in a colab notebook, like I've done with a Pictionary sketch recognition model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwNP5DJOKUql"
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='https://hf.space/gradioiframe/abidlabs/Draw/+', width=1000, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dw6H-iQAlF8I"
   },
   "source": [
    "**Challenge #3**: Upload your Gradio demo to Hugging Face Spaces and get a permanent URL for it. Share the permanent URL with someone (a colleague, a collaborator, a friend, a user, etc.) -- what kind of feedback do you get on your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqD0O1PKIg3g"
   },
   "source": [
    "## 3. Add your demo to the Hugging Face org for your class or conference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrMObQbwLOHm"
   },
   "source": [
    "#### **Setup** (for instructors or conference organizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_45C7MnXNbc0"
   },
   "source": [
    "A. First, create a Hugging Face account if you do not already have one, by visiting https://huggingface.co/ and clicking \"Sign Up\"\n",
    "\n",
    "B. Once you are logged in, click on your profile picture and then click on \"New Organization\" underneath it to get to this page: https://huggingface.co/organizations/new\n",
    "\n",
    "C. Fill out the information for your class or conference. We recommend creating a separate organization each time that a class is taught (for example, \"Stanford-CS236g-20222\") and for each year of the conference.\n",
    "\n",
    "D. Your organization will be created and now now users will be able request adding themselves to your organizations by visiting the organization page.\n",
    "\n",
    "E. Optionally, you can change the settings by clicking on the \"Organization settings\" button. Typically, for classes and conferences, you will want to navigate to `Settings > Members` and set the \"Default role for new members\" to be \"write\", which allows them to submit Spaces but not change the settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSqzO-w8LY0R"
   },
   "source": [
    "#### For students or conference participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x1Oyh4wOdOK"
   },
   "source": [
    "A. Ask your instructor / coneference organizer for the link to the Organization page if you do not already have it\n",
    "\n",
    "B. Visit the Organization page and click \"Request to join this org\" button, if you are not yet part of the org.\n",
    "\n",
    "C. Then, once you have been approved to join the organization (and built your Gradio Demo and uploaded it to Spaces -- see Sections 1 and 2), then simply go to your Space and go to `Settings > Rename or transfer this space` and then select the organization name under `New owner`. Click the button and the Space will now be added to your class or conference Space!   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Building and Hosting Machine Learning Demos with Gradio & Hugging Face",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}